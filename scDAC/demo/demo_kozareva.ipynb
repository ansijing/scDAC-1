{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from os.path import join as pj \n",
    "import time\n",
    "import argparse \n",
    "\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn, autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "from modules import model2, utils\n",
    "from modules.datasets import MultimodalDataset\n",
    "from modules.datasets import MultiDatasetSampler\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "## Task\n",
    "parser.add_argument('--task', type=str, default='Kozareva_total',\n",
    "    help=\"Choose a task\")\n",
    "parser.add_argument('--reference', type=str, default='',\n",
    "    help=\"Choose a reference task\")\n",
    "parser.add_argument('--exp', type=str, default='e0',\n",
    "    help=\"Choose an experiment\")\n",
    "parser.add_argument('--model', type=str, default='default',\n",
    "    help=\"Choose a model configuration\")\n",
    "# parser.add_argument('--data', type=str, default='sup',\n",
    "#     help=\"Choose a data configuration\")\n",
    "parser.add_argument('--action', type=str, default='train',\n",
    "    help=\"Choose an action to run\")\n",
    "parser.add_argument('--method', type=str, default='scDAC',\n",
    "    help=\"Choose an method to benchmark\")\n",
    "parser.add_argument('--init-model', type=str, default='',\n",
    "    help=\"Load a saved model\")\n",
    "parser.add_argument('--mods-conditioned', type=str, nargs='+', default=[],\n",
    "    help=\"Modalities conditioned for sampling\")\n",
    "parser.add_argument('--data-conditioned', type=str, default='prior.csv',\n",
    "    help=\"Data conditioned for sampling\")\n",
    "parser.add_argument('--sample-num', type=int, default=0,\n",
    "    help='Number of samples to be generated')\n",
    "parser.add_argument('--input-mods', type=str, nargs='+', default=[],\n",
    "    help=\"Input modalities for transformation\")\n",
    "## Training\n",
    "parser.add_argument('--epoch-num', type=int, default=500,\n",
    "    help='Number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "    help='Learning rate')\n",
    "#parser.add_argument('--dim_logitx', type=int, default=64,\n",
    "#    help='dim_logitx')\n",
    "parser.add_argument('--grad-clip', type=float, default=-1,\n",
    "    help='Gradient clipping value')\n",
    "parser.add_argument('--s-drop-rate', type=float, default=0.1,\n",
    "    help=\"Probility of dropping out subject ID during training\")\n",
    "parser.add_argument('--seed', type=int, default=-1,\n",
    "    help=\"Set the random seed to reproduce the results\")\n",
    "parser.add_argument('--use-shm', type=int, default=1,\n",
    "    help=\"Use shared memory to accelerate training\")\n",
    "## Debugging\n",
    "parser.add_argument('--print-iters', type=int, default=-1,\n",
    "    help=\"Iterations to print training messages\")\n",
    "parser.add_argument('--log-epochs', type=int, default=100,\n",
    "    help='Epochs to log the training states')\n",
    "parser.add_argument('--save-epochs', type=int, default=1,\n",
    "    help='Epochs to save the latest training states (overwrite previous ones)')\n",
    "parser.add_argument('--time', type=int, default=0, choices=[0, 1],\n",
    "    help='Time the forward and backward passes')\n",
    "parser.add_argument('--debug', type=int, default=1, choices=[0, 1],\n",
    "    help='Print intermediate variables')\n",
    "# o, _ = parser.parse_known_args()  # for python interactive\n",
    "o = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = None\n",
    "net = None\n",
    "#discriminator = None \n",
    "optimizer_net = None\n",
    "#optimizer_disc = None\n",
    "benchmark = {\n",
    "    \"train_loss\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"foscttm\": [],\n",
    "    \"epoch_id_start\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    initialize()\n",
    "    if o.action == \"print_model\":\n",
    "        print_model() ##\n",
    "    elif o.action == \"train\":\n",
    "        train() #\n",
    "    elif o.action == \"test\":\n",
    "        test() ###\n",
    "    elif o.action == \"infer_latent\":\n",
    "        infer_latent(only_joint=False, impute=False, save_input=True)###\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid action!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    init_seed() ##\n",
    "    init_dirs() ##\n",
    "    load_data_config() ##\n",
    "    load_model_config() ##\n",
    "    get_gpu_config() ##\n",
    "    init_model() ##\n",
    "\n",
    "\n",
    "def init_seed():\n",
    "    if o.seed >= 0:\n",
    "        np.random.seed(o.seed) \n",
    "        th.manual_seed(o.seed) \n",
    "        th.cuda.manual_seed_all(o.seed)\n",
    "\n",
    "\n",
    "def init_dirs():\n",
    "    if o.use_shm == 1:\n",
    "        o.data_dir = pj(\"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data\", \"processed\",  o.task)\n",
    "    else:\n",
    "        o.data_dir = pj(\"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data\", \"processed\", o.task)\n",
    "    o.result_dir = pj(\"result\", o.task, o.exp, o.model)\n",
    "    if o.reference == '': \n",
    "        o.train_dir = pj(\"result\", o.task, o.exp, o.model, \"train\")\n",
    "    else:\n",
    "        o.train_dir = pj(\"result\", o.reference, o.exp, o.model, \"train\")\n",
    "    o.debug_dir = pj(o.result_dir, \"debug\")\n",
    "    utils.mkdirs([o.train_dir, o.debug_dir])\n",
    "    print(\"Task: %s\\nExperiment: %s\\nModel: %s\\n\" % (o.task, o.exp, o.model))\n",
    "\n",
    "\n",
    "def load_data_config():\n",
    "    get_dims_x()\n",
    "    o.mods = list(o.dims_x.keys())\n",
    "    o.mod_num = len(o.dims_x)\n",
    "    global data_config\n",
    "    data_config = utils.load_toml(\"configs/data.toml\")[o.task]\n",
    "    for k, v in data_config.items():\n",
    "        vars(o)[k] = v\n",
    "\n",
    "    o.s_joint, o.combs, o.s, o.dims_s = utils.gen_all_batch_ids(o.s_joint, o.combs)\n",
    "    \n",
    "\n",
    "    \n",
    "    if o.reference != '':\n",
    "        data_config_ref = utils.load_toml(\"configs/data.toml\")[o.reference]\n",
    "        _, _, _, o.dims_s = utils.gen_all_batch_ids(data_config_ref[\"s_joint\"], \n",
    "                                                    data_config_ref[\"combs\"])\n",
    "\n",
    "\n",
    "\n",
    "def load_model_config():\n",
    "    model_config = utils.load_toml(\"configs/model.toml\")[\"default\"]\n",
    "    if o.model != \"default\":\n",
    "        model_config.update(utils.load_toml(\"configs/model.toml\")[o.model])\n",
    "    for k, v in model_config.items():\n",
    "        vars(o)[k] = v\n",
    "    o.dim_z = o.dim_c\n",
    "    o.dims_dec_x = o.dims_enc_x[::-1]\n",
    "    if \"dims_enc_chr\" in vars(o).keys():\n",
    "        o.dims_dec_chr = o.dims_enc_chr[::-1]\n",
    "    o.dims_h = {}\n",
    "    for m, dim in o.dims_x.items():\n",
    "        o.dims_h[m] = dim if m != \"atac\" else o.dims_enc_chr[-1] * 22\n",
    "    print(\"dims_h:\", o.dims_h)\n",
    "\n",
    "def get_gpu_config():\n",
    "    o.G = 1  # th.cuda.device_count()  # get GPU number\n",
    "    o.N = 512\n",
    "    assert o.N % o.G == 0, \"Please ensure the mini-batch size can be divided \" \\\n",
    "        \"by the GPU number\"\n",
    "    o.n = o.N // o.G\n",
    "    print(\"Total mini-batch size: %d, GPU number: %d, GPU mini-batch size: %d\" % (o.N, o.G, o.n))\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"\n",
    "    Initialize the model, optimizer, and benchmark\n",
    "    \"\"\"\n",
    "    global net, optimizer_net\n",
    "    net = model2.Net_DP(o).cuda()\n",
    "    optimizer_net = th.optim.AdamW(net.parameters(), lr=o.lr)\n",
    "    if o.init_model != '':\n",
    "        fpath = pj(o.train_dir, o.init_model)\n",
    "        savepoint = th.load(fpath+\".pt\")\n",
    "        net.load_state_dict(savepoint['net_states'])\n",
    "        optimizer_net.load_state_dict(savepoint['optim_net_states'])\n",
    "        benchmark.update(utils.load_toml(fpath+\".toml\")['benchmark'])\n",
    "        print('Model is initialized from ' + fpath + \".pt\")\n",
    "    net_param_num = sum([param.data.numel() for param in net.parameters()])\n",
    "    print('Parameter number: %.3f M' % (net_param_num / 1e6))\n",
    "\n",
    "\n",
    "def print_model():\n",
    "    #global net, discriminator\n",
    "    global net\n",
    "    with open(pj(o.result_dir, \"model_architecture.txt\"), 'w') as f:\n",
    "        print(net, file=f)\n",
    "\n",
    "\n",
    "def get_dims_x():\n",
    "    dims_x = utils.load_csv(pj(o.data_dir, \"feat\", \"feat_dims.csv\"))\n",
    "    dims_x = utils.transpose_list(dims_x)\n",
    "    o.dims_x = {}\n",
    "    for i in range(1, len(dims_x)):\n",
    "        m = dims_x[i][0]\n",
    "        if m == \"atac\":\n",
    "            o.dims_chr = list(map(int, dims_x[i][1:]))\n",
    "            o.dims_x[m] = sum(o.dims_chr)\n",
    "        else:\n",
    "            o.dims_x[m] = int(dims_x[i][1])\n",
    "\n",
    "\n",
    "    print(\"Input feature numbers: \", o.dims_x)\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_data_loader_cat = get_dataloader_cat(\"train\")\n",
    "    epoch_id_list = []\n",
    "    ari_list = []\n",
    "    nmi_list = []\n",
    "    sc_list = []\n",
    "\n",
    "    for epoch_id in range(benchmark['epoch_id_start'], o.epoch_num):\n",
    "        run_epoch(train_data_loader_cat, \"train\", epoch_id)\n",
    "        if epoch_id >450:\n",
    "\n",
    "            z = infer_latent_dp(save_input=False)\n",
    "            net.loss_calculator_dp.mean_dp, net.loss_calculator_dp.weight_concentration_dp,net.loss_calculator_dp.mean_precision_dp,net.loss_calculator_dp.precisions_cholesky_dp, net.loss_calculator_dp.degrees_of_freedom_dp, net.scdp.predict_label = dp(z)\n",
    "            # ari, nmi, sc = cluster_index_calculer(z, net.scdp.predict_label)\n",
    "            # print(\"ari:\", ari)\n",
    "            # print(\"nmi:\", nmi)\n",
    "            # print(\"sc:\", sc)\n",
    "            epoch_id_list.append(epoch_id)\n",
    "            # ari_list.append(ari)\n",
    "            # nmi_list.append(nmi)\n",
    "            # sc_list.append(sc)\n",
    "            # plt_ari(epoch_id_list, ari_list)\n",
    "            # plt_nmi(epoch_id_list, nmi_list)\n",
    "            # plt_sc(epoch_id_list, sc_list)\n",
    "        else:\n",
    "            pass            \n",
    "        check_to_save(epoch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if o.task == 'chen_10':\n",
    "    path_label = './data/label_chen_10.csv'\n",
    "elif o.task == 'baron_single':\n",
    "    path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/b1/label_seurat/l1.csv'    \n",
    "elif o.task == 'Kozareva_total':\n",
    "    path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/k_t/label_seurat/label1.csv'    \n",
    "elif o.task == 'Orozco':\n",
    "    path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/o1/label_seurat/label.csv' \n",
    "elif o.task == 'Slyper':\n",
    "    path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/s1/label_seurat/label_new.csv' \n",
    "elif o.task == 'Zilionis':\n",
    "    path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/z1/label_seurat/label.csv' \n",
    "else:\n",
    "    path_label = './data/label_chen_8.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp(z):\n",
    "    # z_np = (z).cpu().detach().numpy()   \n",
    "    z_np = z.cpu().detach().numpy()   \n",
    "    bgm = BayesianGaussianMixture(\n",
    "        n_components=50, weight_concentration_prior=1e-10,mean_precision_prior = 80,covariance_type='diag',init_params ='kmeans', max_iter=1000, warm_start = True\n",
    "        ).fit(z_np)\n",
    "    predict_label_array = bgm.predict(z_np)\n",
    "    predict_label_array = bgm.predict(z_np)\n",
    "    predict_label = th.Tensor(np.array(predict_label_array)).unsqueeze(1).cuda()\n",
    "    mean_dp = th.Tensor(np.array(bgm.means_))\n",
    "    weight_concentration_dp = th.Tensor(np.array(bgm.weight_concentration_))\n",
    "    precisions_cholesky_dp = th.Tensor(np.array(bgm.precisions_cholesky_))\n",
    "    degrees_of_freedom_dp = th.Tensor(np.array(bgm.degrees_of_freedom_))\n",
    "    mean_precision_dp = th.Tensor(np.array(bgm.mean_precision_))   \n",
    "    return mean_dp, weight_concentration_dp, mean_precision_dp, precisions_cholesky_dp, degrees_of_freedom_dp, predict_label\n",
    "\n",
    "def dp_infer(z):\n",
    "    # z_np = (z).cpu().detach().numpy()   \n",
    "    z_np = z.cpu().detach().numpy()   \n",
    "    bgm = BayesianGaussianMixture(\n",
    "        n_components=30, weight_concentration_prior=1e-10,mean_precision_prior = 80, covariance_type='diag', init_params ='kmeans', max_iter=1000, warm_start = True\n",
    "        ).fit(z_np)\n",
    "    predict_label_array = bgm.predict(z_np)\n",
    "    predict_label = th.Tensor(np.array(predict_label_array)).unsqueeze(1).cuda()\n",
    "    mean_dp = th.Tensor(np.array(bgm.means_))\n",
    "    weight_concentration_dp = th.Tensor(np.array(bgm.weight_concentration_))\n",
    "    precisions_cholesky_dp = th.Tensor(np.array(bgm.precisions_cholesky_))\n",
    "    degrees_of_freedom_dp = th.Tensor(np.array(bgm.degrees_of_freedom_))\n",
    "    mean_precision_dp = th.Tensor(np.array(bgm.mean_precision_))   \n",
    "    return mean_dp, weight_concentration_dp, mean_precision_dp, precisions_cholesky_dp, degrees_of_freedom_dp, predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(split, train_ratio=None):\n",
    "    data_loaders = {}\n",
    "    for subset in range(len(o.s)):\n",
    "        data_loaders[subset] = get_dataloader(subset, split, train_ratio=train_ratio)\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def get_dataloader(subset, split, train_ratio=None):\n",
    "    dataset = MultimodalDataset(o.task, o.data_dir, subset, split, train_ratio=train_ratio)\n",
    "    shuffle = True if split == \"train\" else False\n",
    "    # shuffle = False\n",
    "    data_loader = th.utils.data.DataLoader(dataset, batch_size=o.N, shuffle=shuffle,\n",
    "                                           num_workers=64, pin_memory=True)\n",
    "    print(\"Subset: %d, modalities %s: %s size: %d\" %\n",
    "          (subset, str(o.combs[subset]), split, dataset.size))\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def get_dataloader_cat(split, train_ratio=None):\n",
    "    datasets = []\n",
    "    for subset in range(len(o.s)):\n",
    "        datasets.append(MultimodalDataset(o.task, o.data_dir, subset, split, train_ratio=train_ratio))\n",
    "        print(\"Subset: %d, modalities %s: %s size: %d\" %  (subset, str(o.combs[subset]), split,\n",
    "            datasets[subset].size))\n",
    "    dataset_cat = th.utils.data.dataset.ConcatDataset(datasets)\n",
    "    shuffle = True if split == \"train\" else False\n",
    "    # shuffle = False\n",
    "    sampler = MultiDatasetSampler(dataset_cat, batch_size=o.N, shuffle=shuffle)\n",
    "    data_loader = th.utils.data.DataLoader(dataset_cat, batch_size=o.N, sampler=sampler, \n",
    "        num_workers=64, pin_memory=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dataloader(train_ratio=False):\n",
    "    data_config_new = utils.copy_dict(data_config)\n",
    "    data_config_new.update({\"combs\": [o.mods], \"comb_ratios\": [1]})\n",
    "    if train_ratio:\n",
    "        data_config_new.update({\"train_ratio\": train_ratio})\n",
    "    dataset = MultimodalDataset(data_config_new, o.mods, \"test\")\n",
    "    data_loader = th.utils.data.DataLoader(dataset, batch_size=o.N,\n",
    "        shuffle=False, num_workers=64, pin_memory=True)\n",
    "    print(\"Eval Dataset %s: test: %d\\n\" % (str(o.mods), dataset.size))\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def test():\n",
    "    data_loaders = get_dataloaders()\n",
    "    run_epoch(data_loaders, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_loader, split, epoch_id=0):\n",
    "    if split == \"train\":\n",
    "        net.train()\n",
    "    elif split == \"test\":\n",
    "        net.eval()\n",
    "    else:\n",
    "        assert False, \"Invalid split: %s\" % split\n",
    "    net.o.epoch_id = epoch_id\n",
    "    loss_total = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        loss = run_iter(split, data)\n",
    "        loss_total += loss\n",
    "        if o.print_iters > 0 and (i+1) % o.print_iters == 0:\n",
    "            print('Epoch: %d/%d, Batch: %d/%d, %s loss: %.3f' % (epoch_id+1,\n",
    "            o.epoch_num, i+1, len(data_loader), split, loss))\n",
    "    loss_avg = loss_total / len(data_loader)\n",
    "    print('Epoch: %d/%d, %s loss: %.3f\\n' % (epoch_id+1, o.epoch_num, split, loss_avg))\n",
    "    benchmark[split+'_loss'].append((float(epoch_id), float(loss_avg)))\n",
    "    return loss_avg\n",
    "\n",
    "\n",
    "\n",
    "def run_iter(split, inputs):\n",
    "    inputs = utils.convert_tensors_to_cuda(inputs)\n",
    "    if split == \"train\":\n",
    "        with autograd.set_detect_anomaly(o.debug == 1):\n",
    "            loss_net = forward_net(inputs)\n",
    "            loss = loss_net\n",
    "            update_net(loss) \n",
    "            \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        with th.no_grad():\n",
    "            loss_net = forward_net(inputs)\n",
    "            loss = loss_net\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def forward_net(inputs):\n",
    "    return net(inputs)\n",
    "\n",
    "\n",
    "def update_net(loss):\n",
    "    update(loss, net, optimizer_net)\n",
    "\n",
    "    \n",
    "\n",
    "def update(loss, model, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if o.grad_clip > 0:\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), o.grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def check_to_save(epoch_id):\n",
    "    if (epoch_id+1) % o.log_epochs == 0 or epoch_id+1 == o.epoch_num:\n",
    "        save_training_states(epoch_id, \"sp_%08d\" % epoch_id)\n",
    "    if (epoch_id+1) % o.save_epochs == 0 or epoch_id+1 == o.epoch_num:\n",
    "        save_training_states(epoch_id, \"sp_latest\")\n",
    "\n",
    "\n",
    "def save_training_states(epoch_id, filename):\n",
    "    benchmark['epoch_id_start'] = epoch_id\n",
    "    utils.save_toml({\"o\": vars(o), \"benchmark\": benchmark}, pj(o.train_dir, filename+\".toml\"))\n",
    "    th.save({\"net_states\": net.state_dict(),\n",
    "             \"optim_net_states\": optimizer_net.state_dict(),\n",
    "            }, pj(o.train_dir, filename+\".pt\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def infer_latent_dp(save_input=False):\n",
    "    print(\"Inferring ...\")\n",
    "    dirs = {}\n",
    "    base_dir = pj(o.result_dir, \"represent\", o.init_model)\n",
    "    data_loaders = get_dataloaders(\"test\", train_ratio=0)\n",
    "    net.eval()\n",
    "    with th.no_grad():\n",
    "        for subset_id, data_loader in data_loaders.items():\n",
    "            print(\"Processing subset %d: %s\" % (subset_id, str(o.combs[subset_id])))\n",
    "            dirs[subset_id] = {\"z\": {}, \"x_r_pre\": {}, \"x\": {}}\n",
    "            dirs[subset_id][\"z\"][\"rna\"] = pj(base_dir, \"subset_\"+str(subset_id), \"z\", \"rna\")\n",
    "            utils.mkdirs(dirs[subset_id][\"z\"][\"rna\"], remove_old=False)          \n",
    "            z_list = []\n",
    "            if save_input:\n",
    "                for m in o.combs[subset_id]:\n",
    "                    dirs[subset_id][\"x\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"x\", m)\n",
    "                    utils.mkdirs(dirs[subset_id][\"x\"][m], remove_old=True)\n",
    "            fname_fmt = utils.get_name_fmt(len(data_loader))+\".csv\"    \n",
    "            for i, data in enumerate(data_loader):\n",
    "                data = utils.convert_tensors_to_cuda(data)\n",
    "                _, z= net.scdp(data)      \n",
    "                z_list.append(z)\n",
    "                z_all = th.cat(z_list, dim = 0)\n",
    "    return(z_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_latent(only_joint=True, impute=False, save_input=False):\n",
    "    print(\"Inferring ...\")\n",
    "    dirs = {}\n",
    "    base_dir = pj(o.result_dir, \"represent\", o.init_model)\n",
    "    data_loaders = get_dataloaders(\"test\", train_ratio=0)\n",
    "    net.eval()\n",
    "    with th.no_grad():\n",
    "        # z_all_large = []\n",
    "        for subset_id, data_loader in data_loaders.items():\n",
    "            print(\"Processing subset %d: %s\" % (subset_id, str(o.combs[subset_id])))\n",
    "            \n",
    "            #dirs[subset_id] = {\"z\": {}, \"x_r\": {}, \"x\": {}, \"w_lis1\": {}}\n",
    "            # dirs[subset_id] = {\"z\": {}, \"x_r_pre\": {}, \"x\": {}, \"y_cat_list\": {}, \"c_ymu\": {}, \"n_covariance2\": {}, \"n_mu\": {}, \"d2\": {}, \"w_covariance3\": {}}\n",
    "            # dirs[subset_id] = {\"z\": {}, \"x_r_pre\": {}, \"x\": {}, \"predict_label\":{}}\n",
    "            dirs[subset_id] = {\"z\": {}, \"x_r_pre\": {}, \"x\": {}}\n",
    "            dirs[subset_id][\"z\"][\"joint\"] = pj(base_dir, \"subset_\"+str(subset_id), \"z\", \"joint\")\n",
    "            utils.mkdirs(dirs[subset_id][\"z\"][\"joint\"], remove_old=True)\n",
    "            if not only_joint:\n",
    "                for m in o.combs[subset_id]:\n",
    "                    dirs[subset_id][\"z\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"z\", m)\n",
    "                    utils.mkdirs(dirs[subset_id][\"z\"][m], remove_old=True)\n",
    "                    # dirs[subset_id][\"predict_label\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"predict_label\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"predict_label\"][m], remove_old=True)\n",
    "                    dirs[subset_id][\"x_r_pre\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"x_r_pre\", m)\n",
    "                    utils.mkdirs(dirs[subset_id][\"x_r_pre\"][m], remove_old=True)\n",
    "                    # dirs[subset_id][\"c_ymu\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"c_ymu\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"c_ymu\"][m], remove_old=True)     \n",
    "                    # dirs[subset_id][\"n_covariance2\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"n_covariance2\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"n_covariance2\"][m], remove_old=True)  \n",
    "                    # dirs[subset_id][\"n_mu\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"n_mu\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"n_mu\"][m], remove_old=True)         \n",
    "                    # dirs[subset_id][\"d2\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"d2\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"d2\"][m], remove_old=True)  \n",
    "                    # dirs[subset_id][\"w_covariance3\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"w_covariance3\", m)\n",
    "                    # utils.mkdirs(dirs[subset_id][\"w_covariance3\"][m], remove_old=True)            \n",
    "            if impute:\n",
    "                for m in o.mods:\n",
    "                    dirs[subset_id][\"x_r\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"x_r\", m)\n",
    "                    utils.mkdirs(dirs[subset_id][\"x_r\"][m], remove_old=True)\n",
    "            if save_input:\n",
    "                for m in o.combs[subset_id]:\n",
    "                    dirs[subset_id][\"x\"][m] = pj(base_dir, \"subset_\"+str(subset_id), \"x\", m)\n",
    "                    utils.mkdirs(dirs[subset_id][\"x\"][m], remove_old=True)\n",
    "            fname_fmt = utils.get_name_fmt(len(data_loader))+\".csv\"\n",
    "            \n",
    "            for i, data in enumerate(tqdm(data_loader)):\n",
    "                data = utils.convert_tensors_to_cuda(data)\n",
    "                # conditioned on all observed modalities\n",
    "                #x_r_pre, _, _, _, z, _, _, *_ = net.dpmm(data)  # N * K\n",
    "                x_r_pre, z= net.scdp(data) \n",
    "                utils.save_tensor_to_csv(z, pj(dirs[subset_id][\"z\"][\"joint\"], fname_fmt) % i)\n",
    "                # utils.save_tensor_to_csv(x_r_pre[m], pj(dirs[subset_id][\"x_r_pre\"][m], fname_fmt) % i)\n",
    "                # utils.save_tensor_to_csv(predict_label, pj(dirs[subset_id][\"predict_label\"], fname_fmt) % i)\n",
    "                if impute:\n",
    "                    x_r = model2.gen_real_data(x_r_pre, sampling=True)\n",
    "                    for m in o.mods:\n",
    "                        utils.save_tensor_to_csv(x_r[m], pj(dirs[subset_id][\"x_r\"][m], fname_fmt) % i)\n",
    "                if save_input:\n",
    "                    for m in o.combs[subset_id]:\n",
    "                        utils.save_tensor_to_csv(data[\"x\"][m], pj(dirs[subset_id][\"x\"][m], fname_fmt) % i)\n",
    "\n",
    "                # conditioned on each individual modalities\n",
    "                if not only_joint:\n",
    "                    for m in data[\"x\"].keys():\n",
    "                        input_data = {\n",
    "                            \"x\": {m: data[\"x\"][m]},\n",
    "                            \"s\": data[\"s\"],  #这儿可能得删掉，\n",
    "                            \"e\": {}\n",
    "                        }\n",
    "                        if m in data[\"e\"].keys():\n",
    "                            input_data[\"e\"][m] = data[\"e\"][m]\n",
    "                        #_, _, _, _, z, c, b, *_ = net.sct(input_data)  # N * K\n",
    "                        # _, c_ymu, _, _, _, z, y_cat_list, _, _, _, n_covariance2, n_mu, d2, w_covariance3, _ = net.dpmm(input_data)  # N * K\n",
    "                        _, z= net.scdp(input_data)  # N * K\n",
    "                    \n",
    "                        # print(input_data['x']['rna'].shape, z.shape)\n",
    "                        utils.save_tensor_to_csv(z, pj(dirs[subset_id][\"z\"][m], fname_fmt) % i)    \n",
    "                        # utils.save_tensor_to_csv(predict_label, pj(dirs[subset_id][\"predict_label\"][m], fname_fmt) % i)  \n",
    "                    if i >0:\n",
    "                        z_all = th.cat((z_all, z), dim = 0)\n",
    "                    else:\n",
    "                        z_all = z\n",
    "            # z_all_large.append(z_all)\n",
    "        _, _, _, _, _, predict_label = dp_infer(z_all)\n",
    "        if o.task == 'chen_10':\n",
    "            utils.save_tensor_to_csv(z_all, './result/chen_10/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/chen_10/e0/default/represent/y.csv')   \n",
    "        elif o.task == 'baron_single':\n",
    "            utils.save_tensor_to_csv(z_all, './result/baron_single/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/baron_single/e0/default/represent/y.csv')    \n",
    "        elif o.task == 'Kozareva_total':\n",
    "            utils.save_tensor_to_csv(z_all, './result/Kozareva_total/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/Kozareva_total/e0/default/represent/y.csv')   \n",
    "        elif o.task == 'Orozco':\n",
    "            utils.save_tensor_to_csv(z_all, './result/Orozco/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/Orozco/e0/default/represent/y.csv')   \n",
    "        elif o.task == 'Slyper':\n",
    "            utils.save_tensor_to_csv(z_all, './result/Slyper/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/Slyper/e0/default/represent/y.csv')   \n",
    "        elif o.task == 'Zilionis':\n",
    "            utils.save_tensor_to_csv(z_all, './result/Zilionis/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/Zilionis/e0/default/represent/y.csv')   \n",
    "        else:\n",
    "            utils.save_tensor_to_csv(z_all, './result/chen_10/e0/default/represent/z.csv')\n",
    "            utils.save_tensor_to_csv(predict_label, './result/chen_10/e0/default/represent/y.csv')   \n",
    "        # utils.save_tensor_to_csv(z_all, './result/human_pancreasn8/e0/default/represent/z.csv')\n",
    "        # utils.save_tensor_to_csv(predict_label, './result/human_pancreasn8/e0/default/represent/y.csv')           \n",
    "                # print(\"predict_label\", predict_label1[0:8])\n",
    "                # print(\"predict_label\", predict_label1.size())\n",
    "                # predict_label_list = utils.convert_tensor_to_list((th.argmax(predict_label1, dim = 1).unsqueeze(1)).type_as(predict_label1))\n",
    "                # print(\"predict_labellist:\", predict_label_list[1:10]) \n",
    "        # predict_label_list = utils.convert_tensor_to_list(predict_label)\n",
    "            # if o.task == 'chen_10':\n",
    "            #     utils.save_list_to_csv(predict_label_list, \"./data/chen_10/predict_label.csv\")\n",
    "            #     # path_label = '/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/label2_10.csv'\n",
    "            # elif o.task == 'tabula2_8':\n",
    "            #     utils.save_list_to_csv(predict_label_list, \"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/predict_label2_80224less.csv\")\n",
    "            # elif o.task == 'tabula2_6':\n",
    "            #     utils.save_list_to_csv(predict_label_list, \"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/predict_label2_60224less.csv\")   \n",
    "            # elif o.task == 'tabula2_4':\n",
    "            #     utils.save_list_to_csv(predict_label_list, \"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/predict_label2_4save.csv\") \n",
    "            # else:\n",
    "            #     utils.save_list_to_csv(predict_label_list, \"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/predict_label2_2save.csv\")          \n",
    "            # utils.save_list_to_csv(predict_label_list, \"/root/data/asj/2023/0118/sc-transformer-gmvaextoytoz/data/tab1/label_seurat/predict_label02093.csv\")\n",
    "                # print(\"data_list:\", data_list[1:8])\n",
    "                # if i > 33:\n",
    "\n",
    "        z_all_cpu = z_all.cpu()        \n",
    "        predict_label_cpu = predict_label.cpu()\n",
    "        label_true = utils.load_csv(path_label)\n",
    "        label_tlist = utils.transpose_list(label_true)[1][1:]\n",
    "        label_plist = utils.transpose_list(predict_label_cpu)[0]\n",
    "        ari = adjusted_rand_score(label_tlist, label_plist) #l1 kpca20\n",
    "        nmi = normalized_mutual_info_score(label_tlist, label_plist)\n",
    "        sc = silhouette_score(z_all_cpu, label_plist)\n",
    "        print(\"ari:\", ari)\n",
    "        print(\"nmi:\", nmi)\n",
    "        print(\"sc:\", (1+sc)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Kozareva_total\n",
      "Experiment: e0\n",
      "Model: default\n",
      "\n",
      "Input feature numbers:  {'rna': 4000}\n",
      "dims_h: {'rna': 4000}\n",
      "Total mini-batch size: 512, GPU number: 1, GPU mini-batch size: 512\n",
      "Parameter number: 2.128 M\n",
      "Subset: 0, modalities ['rna']: train size: 611034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, train loss: 1763.232\n",
      "\n",
      "Epoch: 2/500, train loss: 1087.472\n",
      "\n",
      "Epoch: 3/500, train loss: 1040.613\n",
      "\n",
      "Epoch: 4/500, train loss: 1016.115\n",
      "\n",
      "Epoch: 5/500, train loss: 1001.282\n",
      "\n",
      "Epoch: 6/500, train loss: 991.444\n",
      "\n",
      "Epoch: 7/500, train loss: 983.941\n",
      "\n",
      "Epoch: 8/500, train loss: 978.326\n",
      "\n",
      "Epoch: 9/500, train loss: 973.909\n",
      "\n",
      "Epoch: 10/500, train loss: 970.135\n",
      "\n",
      "Epoch: 11/500, train loss: 967.559\n",
      "\n",
      "Epoch: 12/500, train loss: 964.550\n",
      "\n",
      "Epoch: 13/500, train loss: 962.861\n",
      "\n",
      "Epoch: 14/500, train loss: 961.013\n",
      "\n",
      "Epoch: 15/500, train loss: 959.417\n",
      "\n",
      "Epoch: 16/500, train loss: 958.021\n",
      "\n",
      "Epoch: 17/500, train loss: 956.760\n",
      "\n",
      "Epoch: 18/500, train loss: 955.405\n",
      "\n",
      "Epoch: 19/500, train loss: 954.199\n",
      "\n",
      "Epoch: 20/500, train loss: 953.244\n",
      "\n",
      "Epoch: 21/500, train loss: 952.475\n",
      "\n",
      "Epoch: 22/500, train loss: 951.620\n",
      "\n",
      "Epoch: 23/500, train loss: 951.102\n",
      "\n",
      "Epoch: 24/500, train loss: 950.401\n",
      "\n",
      "Epoch: 25/500, train loss: 949.527\n",
      "\n",
      "Epoch: 26/500, train loss: 948.824\n",
      "\n",
      "Epoch: 27/500, train loss: 948.451\n",
      "\n",
      "Epoch: 28/500, train loss: 947.756\n",
      "\n",
      "Epoch: 29/500, train loss: 947.171\n",
      "\n",
      "Epoch: 30/500, train loss: 946.906\n",
      "\n",
      "Epoch: 31/500, train loss: 946.114\n",
      "\n",
      "Epoch: 32/500, train loss: 945.882\n",
      "\n",
      "Epoch: 33/500, train loss: 945.420\n",
      "\n",
      "Epoch: 34/500, train loss: 944.985\n",
      "\n",
      "Epoch: 35/500, train loss: 944.446\n",
      "\n",
      "Epoch: 36/500, train loss: 944.475\n",
      "\n",
      "Epoch: 37/500, train loss: 943.283\n",
      "\n",
      "Epoch: 38/500, train loss: 943.424\n",
      "\n",
      "Epoch: 39/500, train loss: 942.620\n",
      "\n",
      "Epoch: 40/500, train loss: 942.460\n",
      "\n",
      "Epoch: 41/500, train loss: 941.970\n",
      "\n",
      "Epoch: 42/500, train loss: 941.767\n",
      "\n",
      "Epoch: 43/500, train loss: 941.347\n",
      "\n",
      "Epoch: 44/500, train loss: 941.180\n",
      "\n",
      "Epoch: 45/500, train loss: 941.013\n",
      "\n",
      "Epoch: 46/500, train loss: 940.184\n",
      "\n",
      "Epoch: 47/500, train loss: 939.917\n",
      "\n",
      "Epoch: 48/500, train loss: 940.044\n",
      "\n",
      "Epoch: 49/500, train loss: 939.661\n",
      "\n",
      "Epoch: 50/500, train loss: 938.898\n",
      "\n",
      "Epoch: 51/500, train loss: 938.972\n",
      "\n",
      "Epoch: 52/500, train loss: 938.607\n",
      "\n",
      "Epoch: 53/500, train loss: 938.593\n",
      "\n",
      "Epoch: 54/500, train loss: 938.206\n",
      "\n",
      "Epoch: 55/500, train loss: 938.172\n",
      "\n",
      "Epoch: 56/500, train loss: 937.750\n",
      "\n",
      "Epoch: 57/500, train loss: 937.659\n",
      "\n",
      "Epoch: 58/500, train loss: 937.643\n",
      "\n",
      "Epoch: 59/500, train loss: 937.291\n",
      "\n",
      "Epoch: 60/500, train loss: 937.158\n",
      "\n",
      "Epoch: 61/500, train loss: 936.309\n",
      "\n",
      "Epoch: 62/500, train loss: 936.363\n",
      "\n",
      "Epoch: 63/500, train loss: 936.231\n",
      "\n",
      "Epoch: 64/500, train loss: 936.154\n",
      "\n",
      "Epoch: 65/500, train loss: 935.628\n",
      "\n",
      "Epoch: 66/500, train loss: 935.631\n",
      "\n",
      "Epoch: 67/500, train loss: 935.286\n",
      "\n",
      "Epoch: 68/500, train loss: 935.064\n",
      "\n",
      "Epoch: 69/500, train loss: 934.878\n",
      "\n",
      "Epoch: 70/500, train loss: 934.591\n",
      "\n",
      "Epoch: 71/500, train loss: 934.365\n",
      "\n",
      "Epoch: 72/500, train loss: 934.276\n",
      "\n",
      "Epoch: 73/500, train loss: 934.036\n",
      "\n",
      "Epoch: 74/500, train loss: 933.710\n",
      "\n",
      "Epoch: 75/500, train loss: 934.038\n",
      "\n",
      "Epoch: 76/500, train loss: 933.171\n",
      "\n",
      "Epoch: 77/500, train loss: 933.184\n",
      "\n",
      "Epoch: 78/500, train loss: 932.922\n",
      "\n",
      "Epoch: 79/500, train loss: 932.705\n",
      "\n",
      "Epoch: 80/500, train loss: 932.939\n",
      "\n",
      "Epoch: 81/500, train loss: 932.782\n",
      "\n",
      "Epoch: 82/500, train loss: 932.610\n",
      "\n",
      "Epoch: 83/500, train loss: 932.096\n",
      "\n",
      "Epoch: 84/500, train loss: 931.721\n",
      "\n",
      "Epoch: 85/500, train loss: 931.749\n",
      "\n",
      "Epoch: 86/500, train loss: 931.609\n",
      "\n",
      "Epoch: 87/500, train loss: 931.445\n",
      "\n",
      "Epoch: 88/500, train loss: 931.263\n",
      "\n",
      "Epoch: 89/500, train loss: 931.036\n",
      "\n",
      "Epoch: 90/500, train loss: 931.039\n",
      "\n",
      "Epoch: 91/500, train loss: 930.845\n",
      "\n",
      "Epoch: 92/500, train loss: 930.332\n",
      "\n",
      "Epoch: 93/500, train loss: 930.368\n",
      "\n",
      "Epoch: 94/500, train loss: 930.363\n",
      "\n",
      "Epoch: 95/500, train loss: 929.942\n",
      "\n",
      "Epoch: 96/500, train loss: 929.991\n",
      "\n",
      "Epoch: 97/500, train loss: 929.877\n",
      "\n",
      "Epoch: 98/500, train loss: 929.881\n",
      "\n",
      "Epoch: 99/500, train loss: 929.129\n",
      "\n",
      "Epoch: 100/500, train loss: 929.211\n",
      "\n",
      "Epoch: 101/500, train loss: 928.862\n",
      "\n",
      "Epoch: 102/500, train loss: 928.938\n",
      "\n",
      "Epoch: 103/500, train loss: 928.964\n",
      "\n",
      "Epoch: 104/500, train loss: 928.623\n",
      "\n",
      "Epoch: 105/500, train loss: 928.432\n",
      "\n",
      "Epoch: 106/500, train loss: 928.353\n",
      "\n",
      "Epoch: 107/500, train loss: 928.244\n",
      "\n",
      "Epoch: 108/500, train loss: 928.101\n",
      "\n",
      "Epoch: 109/500, train loss: 928.010\n",
      "\n",
      "Epoch: 110/500, train loss: 927.660\n",
      "\n",
      "Epoch: 111/500, train loss: 927.632\n",
      "\n",
      "Epoch: 112/500, train loss: 927.870\n",
      "\n",
      "Epoch: 113/500, train loss: 927.317\n",
      "\n",
      "Epoch: 114/500, train loss: 927.496\n",
      "\n",
      "Epoch: 115/500, train loss: 927.143\n",
      "\n",
      "Epoch: 116/500, train loss: 926.571\n",
      "\n",
      "Epoch: 117/500, train loss: 926.961\n",
      "\n",
      "Epoch: 118/500, train loss: 926.721\n",
      "\n",
      "Epoch: 119/500, train loss: 926.489\n",
      "\n",
      "Epoch: 120/500, train loss: 926.626\n",
      "\n",
      "Epoch: 121/500, train loss: 926.397\n",
      "\n",
      "Epoch: 122/500, train loss: 926.056\n",
      "\n",
      "Epoch: 123/500, train loss: 926.174\n",
      "\n",
      "Epoch: 124/500, train loss: 925.926\n",
      "\n",
      "Epoch: 125/500, train loss: 925.798\n",
      "\n",
      "Epoch: 126/500, train loss: 925.592\n",
      "\n",
      "Epoch: 127/500, train loss: 925.469\n",
      "\n",
      "Epoch: 128/500, train loss: 925.550\n",
      "\n",
      "Epoch: 129/500, train loss: 925.286\n",
      "\n",
      "Epoch: 130/500, train loss: 924.947\n",
      "\n",
      "Epoch: 131/500, train loss: 924.932\n",
      "\n",
      "Epoch: 132/500, train loss: 924.970\n",
      "\n",
      "Epoch: 133/500, train loss: 924.886\n",
      "\n",
      "Epoch: 134/500, train loss: 924.828\n",
      "\n",
      "Epoch: 135/500, train loss: 924.643\n",
      "\n",
      "Epoch: 136/500, train loss: 924.666\n",
      "\n",
      "Epoch: 137/500, train loss: 924.251\n",
      "\n",
      "Epoch: 138/500, train loss: 923.977\n",
      "\n",
      "Epoch: 139/500, train loss: 923.946\n",
      "\n",
      "Epoch: 140/500, train loss: 923.771\n",
      "\n",
      "Epoch: 141/500, train loss: 923.710\n",
      "\n",
      "Epoch: 142/500, train loss: 923.578\n",
      "\n",
      "Epoch: 143/500, train loss: 923.500\n",
      "\n",
      "Epoch: 144/500, train loss: 923.335\n",
      "\n",
      "Epoch: 145/500, train loss: 923.497\n",
      "\n",
      "Epoch: 146/500, train loss: 923.481\n",
      "\n",
      "Epoch: 147/500, train loss: 923.152\n",
      "\n",
      "Epoch: 148/500, train loss: 922.872\n",
      "\n",
      "Epoch: 149/500, train loss: 923.058\n",
      "\n",
      "Epoch: 150/500, train loss: 923.067\n",
      "\n",
      "Epoch: 151/500, train loss: 922.766\n",
      "\n",
      "Epoch: 152/500, train loss: 922.707\n",
      "\n",
      "Epoch: 153/500, train loss: 922.366\n",
      "\n",
      "Epoch: 154/500, train loss: 922.376\n",
      "\n",
      "Epoch: 155/500, train loss: 922.354\n",
      "\n",
      "Epoch: 156/500, train loss: 921.973\n",
      "\n",
      "Epoch: 157/500, train loss: 922.101\n",
      "\n",
      "Epoch: 158/500, train loss: 922.071\n",
      "\n",
      "Epoch: 159/500, train loss: 921.711\n",
      "\n",
      "Epoch: 160/500, train loss: 921.749\n",
      "\n",
      "Epoch: 161/500, train loss: 921.697\n",
      "\n",
      "Epoch: 162/500, train loss: 921.312\n",
      "\n",
      "Epoch: 163/500, train loss: 921.288\n",
      "\n",
      "Epoch: 164/500, train loss: 921.419\n",
      "\n",
      "Epoch: 165/500, train loss: 921.144\n",
      "\n",
      "Epoch: 166/500, train loss: 921.166\n",
      "\n",
      "Epoch: 167/500, train loss: 920.864\n",
      "\n",
      "Epoch: 168/500, train loss: 920.696\n",
      "\n",
      "Epoch: 169/500, train loss: 920.695\n",
      "\n",
      "Epoch: 170/500, train loss: 920.796\n",
      "\n",
      "Epoch: 171/500, train loss: 920.631\n",
      "\n",
      "Epoch: 172/500, train loss: 920.599\n",
      "\n",
      "Epoch: 173/500, train loss: 920.370\n",
      "\n",
      "Epoch: 174/500, train loss: 920.307\n",
      "\n",
      "Epoch: 175/500, train loss: 920.239\n",
      "\n",
      "Epoch: 176/500, train loss: 920.073\n",
      "\n",
      "Epoch: 177/500, train loss: 920.091\n",
      "\n",
      "Epoch: 178/500, train loss: 920.040\n",
      "\n",
      "Epoch: 179/500, train loss: 919.879\n",
      "\n",
      "Epoch: 180/500, train loss: 919.679\n",
      "\n",
      "Epoch: 181/500, train loss: 919.550\n",
      "\n",
      "Epoch: 182/500, train loss: 919.665\n",
      "\n",
      "Epoch: 183/500, train loss: 919.316\n",
      "\n",
      "Epoch: 184/500, train loss: 919.270\n",
      "\n",
      "Epoch: 185/500, train loss: 919.320\n",
      "\n",
      "Epoch: 186/500, train loss: 919.142\n",
      "\n",
      "Epoch: 187/500, train loss: 919.127\n",
      "\n",
      "Epoch: 188/500, train loss: 918.912\n",
      "\n",
      "Epoch: 189/500, train loss: 919.012\n",
      "\n",
      "Epoch: 190/500, train loss: 918.938\n",
      "\n",
      "Epoch: 191/500, train loss: 918.919\n",
      "\n",
      "Epoch: 192/500, train loss: 918.927\n",
      "\n",
      "Epoch: 193/500, train loss: 918.777\n",
      "\n",
      "Epoch: 194/500, train loss: 918.433\n",
      "\n",
      "Epoch: 195/500, train loss: 918.438\n",
      "\n",
      "Epoch: 196/500, train loss: 918.579\n",
      "\n",
      "Epoch: 197/500, train loss: 918.248\n",
      "\n",
      "Epoch: 198/500, train loss: 918.419\n",
      "\n",
      "Epoch: 199/500, train loss: 918.206\n",
      "\n",
      "Epoch: 200/500, train loss: 918.187\n",
      "\n",
      "Epoch: 201/500, train loss: 918.105\n",
      "\n",
      "Epoch: 202/500, train loss: 917.945\n",
      "\n",
      "Epoch: 203/500, train loss: 917.763\n",
      "\n",
      "Epoch: 204/500, train loss: 918.037\n",
      "\n",
      "Epoch: 205/500, train loss: 917.834\n",
      "\n",
      "Epoch: 206/500, train loss: 917.614\n",
      "\n",
      "Epoch: 207/500, train loss: 917.421\n",
      "\n",
      "Epoch: 208/500, train loss: 917.376\n",
      "\n",
      "Epoch: 209/500, train loss: 917.434\n",
      "\n",
      "Epoch: 210/500, train loss: 917.414\n",
      "\n",
      "Epoch: 211/500, train loss: 917.374\n",
      "\n",
      "Epoch: 212/500, train loss: 917.268\n",
      "\n",
      "Epoch: 213/500, train loss: 917.169\n",
      "\n",
      "Epoch: 214/500, train loss: 917.098\n",
      "\n",
      "Epoch: 215/500, train loss: 916.846\n",
      "\n",
      "Epoch: 216/500, train loss: 917.011\n",
      "\n",
      "Epoch: 217/500, train loss: 916.998\n",
      "\n",
      "Epoch: 218/500, train loss: 916.789\n",
      "\n",
      "Epoch: 219/500, train loss: 916.733\n",
      "\n",
      "Epoch: 220/500, train loss: 916.708\n",
      "\n",
      "Epoch: 221/500, train loss: 916.659\n",
      "\n",
      "Epoch: 222/500, train loss: 916.505\n",
      "\n",
      "Epoch: 223/500, train loss: 916.445\n",
      "\n",
      "Epoch: 224/500, train loss: 916.478\n",
      "\n",
      "Epoch: 225/500, train loss: 916.335\n",
      "\n",
      "Epoch: 226/500, train loss: 916.309\n",
      "\n",
      "Epoch: 227/500, train loss: 915.966\n",
      "\n",
      "Epoch: 228/500, train loss: 916.251\n",
      "\n",
      "Epoch: 229/500, train loss: 915.999\n",
      "\n",
      "Epoch: 230/500, train loss: 915.950\n",
      "\n",
      "Epoch: 231/500, train loss: 915.935\n",
      "\n",
      "Epoch: 232/500, train loss: 915.820\n",
      "\n",
      "Epoch: 233/500, train loss: 915.748\n",
      "\n",
      "Epoch: 234/500, train loss: 915.740\n",
      "\n",
      "Epoch: 235/500, train loss: 915.534\n",
      "\n",
      "Epoch: 236/500, train loss: 915.441\n",
      "\n",
      "Epoch: 237/500, train loss: 915.353\n",
      "\n",
      "Epoch: 238/500, train loss: 915.234\n",
      "\n",
      "Epoch: 239/500, train loss: 915.395\n",
      "\n",
      "Epoch: 240/500, train loss: 914.986\n",
      "\n",
      "Epoch: 241/500, train loss: 915.052\n",
      "\n",
      "Epoch: 242/500, train loss: 914.988\n",
      "\n",
      "Epoch: 243/500, train loss: 914.989\n",
      "\n",
      "Epoch: 244/500, train loss: 914.911\n",
      "\n",
      "Epoch: 245/500, train loss: 914.881\n",
      "\n",
      "Epoch: 246/500, train loss: 914.765\n",
      "\n",
      "Epoch: 247/500, train loss: 914.776\n",
      "\n",
      "Epoch: 248/500, train loss: 914.513\n",
      "\n",
      "Epoch: 249/500, train loss: 914.536\n",
      "\n",
      "Epoch: 250/500, train loss: 914.496\n",
      "\n",
      "Epoch: 251/500, train loss: 914.394\n",
      "\n",
      "Epoch: 252/500, train loss: 914.289\n",
      "\n",
      "Epoch: 253/500, train loss: 914.253\n",
      "\n",
      "Epoch: 254/500, train loss: 914.397\n",
      "\n",
      "Epoch: 255/500, train loss: 914.164\n",
      "\n",
      "Epoch: 256/500, train loss: 914.297\n",
      "\n",
      "Epoch: 257/500, train loss: 914.062\n",
      "\n",
      "Epoch: 258/500, train loss: 913.942\n",
      "\n",
      "Epoch: 259/500, train loss: 914.071\n",
      "\n",
      "Epoch: 260/500, train loss: 913.888\n",
      "\n",
      "Epoch: 261/500, train loss: 913.873\n",
      "\n",
      "Epoch: 262/500, train loss: 913.910\n",
      "\n",
      "Epoch: 263/500, train loss: 913.618\n",
      "\n",
      "Epoch: 264/500, train loss: 913.612\n",
      "\n",
      "Epoch: 265/500, train loss: 913.608\n",
      "\n",
      "Epoch: 266/500, train loss: 913.547\n",
      "\n",
      "Epoch: 267/500, train loss: 913.566\n",
      "\n",
      "Epoch: 268/500, train loss: 913.309\n",
      "\n",
      "Epoch: 269/500, train loss: 913.374\n",
      "\n",
      "Epoch: 270/500, train loss: 913.350\n",
      "\n",
      "Epoch: 271/500, train loss: 913.442\n",
      "\n",
      "Epoch: 272/500, train loss: 913.357\n",
      "\n",
      "Epoch: 273/500, train loss: 913.258\n",
      "\n",
      "Epoch: 274/500, train loss: 913.286\n",
      "\n",
      "Epoch: 275/500, train loss: 913.198\n",
      "\n",
      "Epoch: 276/500, train loss: 913.081\n",
      "\n",
      "Epoch: 277/500, train loss: 913.027\n",
      "\n",
      "Epoch: 278/500, train loss: 912.842\n",
      "\n",
      "Epoch: 279/500, train loss: 913.118\n",
      "\n",
      "Epoch: 280/500, train loss: 912.665\n",
      "\n",
      "Epoch: 281/500, train loss: 912.802\n",
      "\n",
      "Epoch: 282/500, train loss: 912.722\n",
      "\n",
      "Epoch: 283/500, train loss: 912.740\n",
      "\n",
      "Epoch: 284/500, train loss: 912.766\n",
      "\n",
      "Epoch: 285/500, train loss: 912.790\n",
      "\n",
      "Epoch: 286/500, train loss: 912.711\n",
      "\n",
      "Epoch: 287/500, train loss: 912.412\n",
      "\n",
      "Epoch: 288/500, train loss: 912.503\n",
      "\n",
      "Epoch: 289/500, train loss: 912.405\n",
      "\n",
      "Epoch: 290/500, train loss: 912.447\n",
      "\n",
      "Epoch: 291/500, train loss: 912.273\n",
      "\n",
      "Epoch: 292/500, train loss: 912.260\n",
      "\n",
      "Epoch: 293/500, train loss: 912.117\n",
      "\n",
      "Epoch: 294/500, train loss: 912.158\n",
      "\n",
      "Epoch: 295/500, train loss: 912.107\n",
      "\n",
      "Epoch: 296/500, train loss: 912.088\n",
      "\n",
      "Epoch: 297/500, train loss: 911.881\n",
      "\n",
      "Epoch: 298/500, train loss: 911.966\n",
      "\n",
      "Epoch: 299/500, train loss: 912.035\n",
      "\n",
      "Epoch: 300/500, train loss: 911.717\n",
      "\n",
      "Epoch: 301/500, train loss: 911.819\n",
      "\n",
      "Epoch: 302/500, train loss: 911.848\n",
      "\n",
      "Epoch: 303/500, train loss: 911.759\n",
      "\n",
      "Epoch: 304/500, train loss: 911.664\n",
      "\n",
      "Epoch: 305/500, train loss: 911.728\n",
      "\n",
      "Epoch: 306/500, train loss: 911.677\n",
      "\n",
      "Epoch: 307/500, train loss: 911.489\n",
      "\n",
      "Epoch: 308/500, train loss: 911.613\n",
      "\n",
      "Epoch: 309/500, train loss: 911.408\n",
      "\n",
      "Epoch: 310/500, train loss: 911.490\n",
      "\n",
      "Epoch: 311/500, train loss: 911.378\n",
      "\n",
      "Epoch: 312/500, train loss: 911.191\n",
      "\n",
      "Epoch: 313/500, train loss: 911.276\n",
      "\n",
      "Epoch: 314/500, train loss: 911.107\n",
      "\n",
      "Epoch: 315/500, train loss: 911.214\n",
      "\n",
      "Epoch: 316/500, train loss: 911.334\n",
      "\n",
      "Epoch: 317/500, train loss: 911.136\n",
      "\n",
      "Epoch: 318/500, train loss: 911.173\n",
      "\n",
      "Epoch: 319/500, train loss: 910.998\n",
      "\n",
      "Epoch: 320/500, train loss: 911.020\n",
      "\n",
      "Epoch: 321/500, train loss: 910.799\n",
      "\n",
      "Epoch: 322/500, train loss: 910.885\n",
      "\n",
      "Epoch: 323/500, train loss: 911.077\n",
      "\n",
      "Epoch: 324/500, train loss: 910.905\n",
      "\n",
      "Epoch: 325/500, train loss: 910.709\n",
      "\n",
      "Epoch: 326/500, train loss: 910.865\n",
      "\n",
      "Epoch: 327/500, train loss: 910.870\n",
      "\n",
      "Epoch: 328/500, train loss: 910.767\n",
      "\n",
      "Epoch: 329/500, train loss: 910.840\n",
      "\n",
      "Epoch: 330/500, train loss: 910.718\n",
      "\n",
      "Epoch: 331/500, train loss: 910.720\n",
      "\n",
      "Epoch: 332/500, train loss: 910.519\n",
      "\n",
      "Epoch: 333/500, train loss: 910.515\n",
      "\n",
      "Epoch: 334/500, train loss: 910.387\n",
      "\n",
      "Epoch: 335/500, train loss: 910.657\n",
      "\n",
      "Epoch: 336/500, train loss: 910.474\n",
      "\n",
      "Epoch: 337/500, train loss: 910.427\n",
      "\n",
      "Epoch: 338/500, train loss: 910.446\n",
      "\n",
      "Epoch: 339/500, train loss: 910.375\n",
      "\n",
      "Epoch: 340/500, train loss: 910.361\n",
      "\n",
      "Epoch: 341/500, train loss: 910.168\n",
      "\n",
      "Epoch: 342/500, train loss: 910.239\n",
      "\n",
      "Epoch: 343/500, train loss: 910.273\n",
      "\n",
      "Epoch: 344/500, train loss: 910.069\n",
      "\n",
      "Epoch: 345/500, train loss: 910.207\n",
      "\n",
      "Epoch: 346/500, train loss: 910.170\n",
      "\n",
      "Epoch: 347/500, train loss: 910.132\n",
      "\n",
      "Epoch: 348/500, train loss: 909.955\n",
      "\n",
      "Epoch: 349/500, train loss: 909.932\n",
      "\n",
      "Epoch: 350/500, train loss: 910.050\n",
      "\n",
      "Epoch: 351/500, train loss: 909.814\n",
      "\n",
      "Epoch: 352/500, train loss: 909.791\n",
      "\n",
      "Epoch: 353/500, train loss: 909.880\n",
      "\n",
      "Epoch: 354/500, train loss: 909.759\n",
      "\n",
      "Epoch: 355/500, train loss: 909.828\n",
      "\n",
      "Epoch: 356/500, train loss: 909.803\n",
      "\n",
      "Epoch: 357/500, train loss: 909.945\n",
      "\n",
      "Epoch: 358/500, train loss: 909.649\n",
      "\n",
      "Epoch: 359/500, train loss: 909.584\n",
      "\n",
      "Epoch: 360/500, train loss: 909.731\n",
      "\n",
      "Epoch: 361/500, train loss: 909.705\n",
      "\n",
      "Epoch: 362/500, train loss: 909.728\n",
      "\n",
      "Epoch: 363/500, train loss: 909.617\n",
      "\n",
      "Epoch: 364/500, train loss: 909.663\n",
      "\n",
      "Epoch: 365/500, train loss: 909.483\n",
      "\n",
      "Epoch: 366/500, train loss: 909.606\n",
      "\n",
      "Epoch: 367/500, train loss: 909.463\n",
      "\n",
      "Epoch: 368/500, train loss: 909.395\n",
      "\n",
      "Epoch: 369/500, train loss: 909.578\n",
      "\n",
      "Epoch: 370/500, train loss: 909.472\n",
      "\n",
      "Epoch: 371/500, train loss: 909.423\n",
      "\n",
      "Epoch: 372/500, train loss: 909.589\n",
      "\n",
      "Epoch: 373/500, train loss: 909.391\n",
      "\n",
      "Epoch: 374/500, train loss: 909.344\n",
      "\n",
      "Epoch: 375/500, train loss: 909.364\n",
      "\n",
      "Epoch: 376/500, train loss: 909.339\n",
      "\n",
      "Epoch: 377/500, train loss: 909.300\n",
      "\n",
      "Epoch: 378/500, train loss: 909.197\n",
      "\n",
      "Epoch: 379/500, train loss: 909.134\n",
      "\n",
      "Epoch: 380/500, train loss: 909.114\n",
      "\n",
      "Epoch: 381/500, train loss: 909.147\n",
      "\n",
      "Epoch: 382/500, train loss: 909.216\n",
      "\n",
      "Epoch: 383/500, train loss: 909.074\n",
      "\n",
      "Epoch: 384/500, train loss: 909.056\n",
      "\n",
      "Epoch: 385/500, train loss: 909.067\n",
      "\n",
      "Epoch: 386/500, train loss: 909.002\n",
      "\n",
      "Epoch: 387/500, train loss: 908.987\n",
      "\n",
      "Epoch: 388/500, train loss: 908.796\n",
      "\n",
      "Epoch: 389/500, train loss: 908.923\n",
      "\n",
      "Epoch: 390/500, train loss: 908.717\n",
      "\n",
      "Epoch: 391/500, train loss: 908.856\n",
      "\n",
      "Epoch: 392/500, train loss: 908.908\n",
      "\n",
      "Epoch: 393/500, train loss: 908.808\n",
      "\n",
      "Epoch: 394/500, train loss: 908.771\n",
      "\n",
      "Epoch: 395/500, train loss: 908.674\n",
      "\n",
      "Epoch: 396/500, train loss: 908.560\n",
      "\n",
      "Epoch: 397/500, train loss: 908.797\n",
      "\n",
      "Epoch: 398/500, train loss: 908.620\n",
      "\n",
      "Epoch: 399/500, train loss: 908.681\n",
      "\n",
      "Epoch: 400/500, train loss: 908.538\n",
      "\n",
      "Epoch: 401/500, train loss: 908.539\n",
      "\n",
      "Epoch: 402/500, train loss: 908.584\n",
      "\n",
      "Epoch: 403/500, train loss: 908.524\n",
      "\n",
      "Epoch: 404/500, train loss: 908.432\n",
      "\n",
      "Epoch: 405/500, train loss: 908.538\n",
      "\n",
      "Epoch: 406/500, train loss: 908.433\n",
      "\n",
      "Epoch: 407/500, train loss: 908.324\n",
      "\n",
      "Epoch: 408/500, train loss: 908.457\n",
      "\n",
      "Epoch: 409/500, train loss: 908.350\n",
      "\n",
      "Epoch: 410/500, train loss: 908.217\n",
      "\n",
      "Epoch: 411/500, train loss: 908.274\n",
      "\n",
      "Epoch: 412/500, train loss: 908.534\n",
      "\n",
      "Epoch: 413/500, train loss: 908.396\n",
      "\n",
      "Epoch: 414/500, train loss: 908.259\n",
      "\n",
      "Epoch: 415/500, train loss: 908.219\n",
      "\n",
      "Epoch: 416/500, train loss: 908.241\n",
      "\n",
      "Epoch: 417/500, train loss: 908.177\n",
      "\n",
      "Epoch: 418/500, train loss: 908.175\n",
      "\n",
      "Epoch: 419/500, train loss: 908.099\n",
      "\n",
      "Epoch: 420/500, train loss: 908.157\n",
      "\n",
      "Epoch: 421/500, train loss: 907.992\n",
      "\n",
      "Epoch: 422/500, train loss: 908.109\n",
      "\n",
      "Epoch: 423/500, train loss: 908.034\n",
      "\n",
      "Epoch: 424/500, train loss: 908.011\n",
      "\n",
      "Epoch: 425/500, train loss: 908.225\n",
      "\n",
      "Epoch: 426/500, train loss: 908.154\n",
      "\n",
      "Epoch: 427/500, train loss: 908.021\n",
      "\n",
      "Epoch: 428/500, train loss: 907.944\n",
      "\n",
      "Epoch: 429/500, train loss: 908.041\n",
      "\n",
      "Epoch: 430/500, train loss: 907.865\n",
      "\n",
      "Epoch: 431/500, train loss: 907.860\n",
      "\n",
      "Epoch: 432/500, train loss: 907.800\n",
      "\n",
      "Epoch: 433/500, train loss: 907.947\n",
      "\n",
      "Epoch: 434/500, train loss: 907.795\n",
      "\n",
      "Epoch: 435/500, train loss: 907.859\n",
      "\n",
      "Epoch: 436/500, train loss: 907.916\n",
      "\n",
      "Epoch: 437/500, train loss: 907.756\n",
      "\n",
      "Epoch: 438/500, train loss: 907.787\n",
      "\n",
      "Epoch: 439/500, train loss: 907.686\n",
      "\n",
      "Epoch: 440/500, train loss: 907.853\n",
      "\n",
      "Epoch: 441/500, train loss: 907.612\n",
      "\n",
      "Epoch: 442/500, train loss: 907.839\n",
      "\n",
      "Epoch: 443/500, train loss: 907.691\n",
      "\n",
      "Epoch: 444/500, train loss: 907.739\n",
      "\n",
      "Epoch: 445/500, train loss: 907.655\n",
      "\n",
      "Epoch: 446/500, train loss: 907.619\n",
      "\n",
      "Epoch: 447/500, train loss: 907.633\n",
      "\n",
      "Epoch: 448/500, train loss: 907.588\n",
      "\n",
      "Epoch: 449/500, train loss: 907.516\n",
      "\n",
      "Epoch: 450/500, train loss: 907.500\n",
      "\n",
      "Epoch: 451/500, train loss: 907.510\n",
      "\n",
      "Epoch: 452/500, train loss: 907.584\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453/500, train loss: 851.161\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 454/500, train loss: 826.624\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455/500, train loss: 806.331\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 456/500, train loss: 788.301\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 457/500, train loss: 774.234\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 458/500, train loss: 763.946\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 459/500, train loss: 756.320\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 460/500, train loss: 751.256\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 461/500, train loss: 747.038\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 462/500, train loss: 743.838\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 463/500, train loss: 741.262\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 464/500, train loss: 739.379\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 465/500, train loss: 737.734\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 466/500, train loss: 736.472\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 467/500, train loss: 735.700\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 468/500, train loss: 734.452\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n",
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469/500, train loss: 733.835\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470/500, train loss: 733.565\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 471/500, train loss: 733.069\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 472/500, train loss: 732.860\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 473/500, train loss: 732.300\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 474/500, train loss: 732.032\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475/500, train loss: 731.771\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 476/500, train loss: 731.427\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477/500, train loss: 731.347\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 478/500, train loss: 731.255\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 479/500, train loss: 731.110\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480/500, train loss: 730.879\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 481/500, train loss: 730.284\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 482/500, train loss: 730.638\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 483/500, train loss: 730.173\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 484/500, train loss: 730.218\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485/500, train loss: 729.868\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 486/500, train loss: 729.695\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487/500, train loss: 729.705\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 488/500, train loss: 729.530\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489/500, train loss: 729.590\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490/500, train loss: 729.352\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 491/500, train loss: 729.263\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 492/500, train loss: 729.082\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 493/500, train loss: 729.078\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 494/500, train loss: 729.421\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495/500, train loss: 728.894\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 496/500, train loss: 729.068\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 497/500, train loss: 728.745\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 498/500, train loss: 728.782\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499/500, train loss: 728.527\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500, train loss: 728.439\n",
      "\n",
      "Inferring ...\n",
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n"
     ]
    }
   ],
   "source": [
    "initialize()\n",
    "# if o.action == \"print_model\":\n",
    "print_model() ##\n",
    "# elif o.action == \"train\":\n",
    "train() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0, modalities ['rna']: test size: 611034\n",
      "Processing subset 0: ['rna']\n",
      "Removing directory \"result/Kozareva_total/e0/default/represent/subset_0/z/joint\"\n",
      "Removing directory \"result/Kozareva_total/e0/default/represent/subset_0/z/rna\"\n",
      "Removing directory \"result/Kozareva_total/e0/default/represent/subset_0/x_r_pre/rna\"\n",
      "Removing directory \"result/Kozareva_total/e0/default/represent/subset_0/x/rna\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1194/1194 [13:53<00:00,  1.43it/s]\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ari: 0.9940517257162463\n",
      "nmi: 0.959098272052648\n",
      "sc: 0.821046769618988\n"
     ]
    }
   ],
   "source": [
    "infer_latent(only_joint=False, impute=False, save_input=True)###\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
